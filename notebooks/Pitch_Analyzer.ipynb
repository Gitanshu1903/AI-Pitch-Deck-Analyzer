{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Setup: Imports and API Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GITANSHU\\anaconda3\\envs\\langchain_test\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image # For image handling with OCR\n",
    "\n",
    "# --- Import Google Generative AI ---\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "from google.api_core import exceptions as google_exceptions\n",
    "\n",
    "# --- Import PDF to Image Library ---\n",
    "from pdf2image import convert_from_path\n",
    "from pdf2image.exceptions import PDFPageCountError, PDFSyntaxError\n",
    "\n",
    "# --- Configure Logging ---\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-30 18:07:19,596 - INFO - Google Generative AI client configured successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Load API Key ---\n",
    "load_dotenv()\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai_configured = False\n",
    "\n",
    "if not google_api_key:\n",
    "    logging.warning(\"Google API key not found in .env file. Gemini features will fail.\")\n",
    "else:\n",
    "    try:\n",
    "        genai.configure(api_key=google_api_key)\n",
    "        genai_configured = True\n",
    "        logging.info(\"Google Generative AI client configured successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to configure Google Generative AI: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM Models ---\n",
    "LLM_VISION_MODEL = \"gemini-1.5-pro-latest\"\n",
    "LLM_TEXT_MODEL = \"gemini-1.5-flash-latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-30 18:07:20,909 - INFO - Analysis focused on 6 sections with weights summing to 100.\n"
     ]
    }
   ],
   "source": [
    "# --- Analysis Configuration ---\n",
    "TARGET_SECTIONS = [\n",
    "    \"Problem\",\n",
    "    \"Solution\",\n",
    "    \"Market Size\",\n",
    "    \"Business Model\",\n",
    "    \"Financial Projections\", \n",
    "    \"Team\"\n",
    "]\n",
    "\n",
    "SECTION_WEIGHTS = {\n",
    "    \"Problem\": 20,\n",
    "    \"Solution\": 20,\n",
    "    \"Market Size\": 13,\n",
    "    \"Business Model\": 13,\n",
    "    \"Financial Projections\": 14,\n",
    "    \"Team\": 20,\n",
    "}\n",
    "logging.info(f\"Analysis focused on {len(TARGET_SECTIONS)} sections with weights summing to {sum(SECTION_WEIGHTS.values())}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORING_CRITERIA = {\n",
    "    \"Problem\": \"\"\"\n",
    "    Evaluate the 'Problem' section based on:\n",
    "    1. Clarity (0-25): Is the problem statement clear, concise, and easy to understand?\n",
    "    2. Magnitude & Significance (0-35): Is the problem significant? Is it painful? Does it affect a large enough audience or have substantial impact? Is this backed by credible data/evidence?\n",
    "    3. Urgency (0-20): Is there a compelling reason why this problem needs to be solved *now*? Market timing?\n",
    "    4. Target Audience Definition (0-20): Is the specific audience facing this problem clearly identified and profiled?\n",
    "    Score (0-100): Provide a single integer score.\n",
    "    Justification: Briefly explain the score (2-3 sentences), mentioning key strengths/weaknesses based on the criteria.\n",
    "    \"\"\",\n",
    "    \"Solution\": \"\"\"\n",
    "    Evaluate the 'Solution' section based on:\n",
    "    1. Clarity & Conciseness (0-25): Is the solution clearly explained and easy to grasp?\n",
    "    2. Problem Fit (0-35): Does the solution directly and effectively address the identified problem?\n",
    "    3. Value Proposition (0-25): Is the unique value for the customer clearly articulated? What makes it compelling and differentiated?\n",
    "    4. Feasibility & Scalability (0-15): Does the solution seem technically feasible? Does it have potential to scale? (Acknowledge early stage limitations).\n",
    "    Score (0-100): Provide a single integer score.\n",
    "    Justification: Briefly explain the score (2-3 sentences).\n",
    "    \"\"\",\n",
    "     \"Market Size\": \"\"\"\n",
    "    Evaluate the 'Market Size' section based on:\n",
    "    1. Market Definition (0-30): Is the target market clearly defined using standard terms (TAM, SAM, SOM)? Is the definition logical?\n",
    "    2. Data & Sources (0-30): Is the market size supported by credible, recent data and sources? Are sources cited?\n",
    "    3. Realism & Focus (0-30): Is the estimation, especially for SOM (Serviceable Obtainable Market), realistic and justifiable for the specific target segment the startup can reach initially?\n",
    "    4. Growth Potential (0-10): Is there an indication of market growth trends?\n",
    "    Score (0-100): Provide a single integer score.\n",
    "    Justification: Briefly explain the score (2-3 sentences).\n",
    "    \"\"\",\n",
    "    \"Business Model\": \"\"\"\n",
    "    Evaluate the 'Business Model' section based on:\n",
    "    1. Clarity of Revenue Streams (0-30): Is the primary way the company makes money clearly explained (e.g., subscription, freemium, transaction fees, licensing)? Are revenue streams distinct?\n",
    "    2. Pricing Strategy (0-25): Is the pricing logical, justified, and aligned with the value proposition?\n",
    "    3. Profitability Path (0-30): Does the model demonstrate a clear potential path to profitability? Are key unit economics (e.g., COGS, Gross Margin, implied LTV/CAC) considered or plausible?\n",
    "    4. Scalability (0-15): Can the business model support significant growth?\n",
    "    Score (0-100): Provide a single integer score.\n",
    "    Justification: Briefly explain the score (2-3 sentences).\n",
    "    \"\"\",\n",
    "    \"Financial Projections\": \"\"\"\n",
    "    Evaluate the 'Financial Projections' section based on:\n",
    "    1. Clarity & Key Metrics (0-25): Are the projections presented clearly? Does it include key financial metrics (revenue, key costs, burn rate, funding needs)?\n",
    "    2. Assumptions & Realism (0-35): Are the underlying assumptions stated or clearly implied? Are they realistic and grounded in the business model, market size, and GTM strategy? Avoid overly optimistic hockey sticks without justification.\n",
    "    3. Time Horizon & Detail (0-20): Does it cover a reasonable time period (e.g., 3-5 years)? Is the level of detail appropriate for the stage?\n",
    "    4. Link to Funding Ask (0-20): Are the projections linked to the amount of funding being requested ('Use of Funds') and the milestones it enables?\n",
    "    Score (0-100): Provide a single integer score.\n",
    "    Justification: Briefly explain the score (2-3 sentences).\n",
    "    \"\"\",\n",
    "    \"Team\": \"\"\"\n",
    "    Evaluate the 'Team' section based on:\n",
    "    1. Founder/Core Team Relevance & Experience (0-40): Do the key team members have highly relevant experience, skills, and domain expertise for this specific venture? Is there founder-market fit?\n",
    "    2. Completeness & Roles (0-25): Are key roles covered for the current stage? Are critical gaps acknowledged (implicitly or explicitly)?\n",
    "    3. Execution Ability & Passion (Deduced) (0-20): Does the presentation convey the team's ability to execute and their passion/commitment? (Partially subjective).\n",
    "    4. Advisors/Board (if applicable) (0-15): Are advisors relevant and do they add significant strategic value? Is their involvement clear?\n",
    "    Score (0-100): Provide a single integer score.\n",
    "    Justification: Briefly explain the score (2-3 sentences).\n",
    "    \"\"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- API Call Configuration ---\n",
    "LLM_TEMPERATURE = 0.3\n",
    "LLM_MAX_TOKENS_OCR = 4096\n",
    "LLM_MAX_TOKENS_SECTION_ID = 500\n",
    "LLM_MAX_TOKENS_SCORING = 350\n",
    "LLM_MAX_TOKENS_FEEDBACK = 1000 \n",
    "LLM_REQUEST_TIMEOUT = 180\n",
    "LLM_RETRY_ATTEMPTS = 3\n",
    "LLM_RETRY_DELAY = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Rate Limit Handling Configuration ---\n",
    "INTER_PAGE_DELAY = 10 \n",
    "RATE_LIMIT_BACKOFF_MULTIPLIER = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Gemini Safety Settings ---\n",
    "SAFETY_SETTINGS_TEXT = { HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE, HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE, HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE, HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE, }\n",
    "SAFETY_SETTINGS_VISION = { HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH, HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH, HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH, HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH, }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Core Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1: Gemini API Call for Vision (OCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini_vision_ocr(image_bytes, mime_type=\"image/jpeg\"):\n",
    "    if not genai_configured: return None\n",
    "    try:\n",
    "        model = genai.GenerativeModel(LLM_VISION_MODEL)\n",
    "        generation_config = genai.GenerationConfig(temperature=0.1, max_output_tokens=LLM_MAX_TOKENS_OCR)\n",
    "        prompt = \"Extract all text visible in this image. Provide only the text content, maintaining layout if possible (e.g., using line breaks).\"\n",
    "        payload = [prompt, {\"mime_type\": mime_type, \"data\": image_bytes}]\n",
    "        for attempt in range(LLM_RETRY_ATTEMPTS):\n",
    "            try:\n",
    "                logging.debug(f\"Calling Gemini Vision (Attempt {attempt + 1}/{LLM_RETRY_ATTEMPTS})\")\n",
    "                response = model.generate_content(contents=payload, generation_config=generation_config, safety_settings=SAFETY_SETTINGS_VISION, request_options={'timeout': LLM_REQUEST_TIMEOUT})\n",
    "                if not response.parts:\n",
    "                    block_reason = response.prompt_feedback.block_reason if response.prompt_feedback else \"Unknown\"\n",
    "                    logging.warning(f\"Gemini Vision response blocked/empty. Reason: {block_reason}\")\n",
    "                    return None\n",
    "                return response.text.strip()\n",
    "            except google_exceptions.GoogleAPIError as e:\n",
    "                 error_str = str(e).lower(); is_rate_limit_error = \"quota\" in error_str or \"rate limit\" in error_str or \"429\" in error_str; suggested_delay_parsed = None\n",
    "                 try:\n",
    "                     if hasattr(e, 'metadata'):\n",
    "                          for item in e.metadata:\n",
    "                               if isinstance(item, tuple) and 'retry-delay' in item[0]:\n",
    "                                    delay_match = re.search(r'seconds:\\s*(\\d+)', str(item[1]));\n",
    "                                    if delay_match: suggested_delay_parsed = int(delay_match.group(1))\n",
    "                 except Exception: pass\n",
    "                 if is_rate_limit_error:\n",
    "                      base_wait = RATE_LIMIT_BACKOFF_MULTIPLIER * (attempt + 1); wait_time = max(suggested_delay_parsed + 2 if suggested_delay_parsed else 0, base_wait)\n",
    "                      logging.warning(f\"Gemini API rate limit/quota error (Attempt {attempt + 1}/{LLM_RETRY_ATTEMPTS}). Waiting {wait_time:.2f}s...\")\n",
    "                 else: wait_time = LLM_RETRY_DELAY * (2 ** attempt); logging.warning(f\"Gemini API error (Attempt {attempt + 1}/{LLM_RETRY_ATTEMPTS}): {e}. Waiting {wait_time:.2f}s...\")\n",
    "                 if attempt == LLM_RETRY_ATTEMPTS - 1: logging.error(f\"Gemini Vision call failed after retries (final error: {e}).\"); return None\n",
    "                 time.sleep(wait_time)\n",
    "            except (google_exceptions.InvalidArgument, ValueError) as e: logging.error(f\"Non-retryable Gemini error (InvalidArgument/ValueError): {e}\"); return None\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Unexpected error during Gemini Vision call (Attempt {attempt + 1}): {e}\")\n",
    "                if attempt < LLM_RETRY_ATTEMPTS - 1: wait_time = LLM_RETRY_DELAY * (2 ** attempt); logging.warning(f\"Retrying unexpected error in {wait_time:.2f}s...\"); time.sleep(wait_time)\n",
    "                else: return None\n",
    "        return None\n",
    "    except Exception as model_init_error: logging.error(f\"Failed to initialize Gemini model '{LLM_VISION_MODEL}': {model_init_error}\"); return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_with_ocr_and_gemini(pdf_path):\n",
    "    pages_text = []; logging.info(f\"Starting PDF processing with Gemini OCR for: {pdf_path}\")\n",
    "    if not genai_configured: return None\n",
    "    try:\n",
    "        logging.info(\"Converting PDF to images...\"); start_conv = time.time(); poppler_path_to_use = None\n",
    "        images = convert_from_path(pdf_path, dpi=200, poppler_path=poppler_path_to_use)\n",
    "        logging.info(f\"Converted {len(images)} pages in {time.time() - start_conv:.2f}s.\")\n",
    "        for i, image in enumerate(images):\n",
    "            page_num = i + 1; logging.info(f\"Processing page {page_num}/{len(images)} with Gemini Vision OCR...\"); start_ocr = time.time()\n",
    "            img_byte_arr = io.BytesIO(); image.save(img_byte_arr, format='JPEG', quality=90)\n",
    "            extracted_text = call_gemini_vision_ocr(img_byte_arr.getvalue(), \"image/jpeg\")\n",
    "            pages_text.append(extracted_text if extracted_text else \"\")\n",
    "            if not extracted_text: logging.warning(f\"Failed/empty OCR for page {page_num}.\")\n",
    "            logging.info(f\"Page {page_num} OCR took {time.time() - start_ocr:.2f}s.\")\n",
    "            if page_num < len(images): logging.info(f\"Waiting {INTER_PAGE_DELAY}s before next page...\"); time.sleep(INTER_PAGE_DELAY)\n",
    "        logging.info(\"Completed Gemini OCR processing.\"); return pages_text\n",
    "    except ImportError: logging.error(\"pdf2image or Pillow not installed.\"); return None\n",
    "    except FileNotFoundError: logging.error(f\"PDF file not found: {pdf_path}\"); return None\n",
    "    except (PDFPageCountError, PDFSyntaxError) as pdf_err: logging.error(f\"PDF conversion error: {pdf_err}. Ensure Poppler is installed/accessible.\"); return None\n",
    "    except Exception as e: logging.error(f\"Unexpected error during PDF/OCR processing: {e}\"); return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3 Gemini API Call for Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini_text(prompt, max_output_tokens=1000, temperature=LLM_TEMPERATURE):\n",
    "    if not genai_configured: return None\n",
    "    try:\n",
    "        model = genai.GenerativeModel(LLM_TEXT_MODEL)\n",
    "        generation_config = genai.GenerationConfig(temperature=temperature, max_output_tokens=max_output_tokens)\n",
    "        for attempt in range(LLM_RETRY_ATTEMPTS):\n",
    "            try:\n",
    "                logging.debug(f\"Calling Gemini Text (Attempt {attempt + 1}/{LLM_RETRY_ATTEMPTS})\")\n",
    "                response = model.generate_content(contents=prompt, generation_config=generation_config, safety_settings=SAFETY_SETTINGS_TEXT, request_options={'timeout': LLM_REQUEST_TIMEOUT})\n",
    "                if not response.parts:\n",
    "                     block_reason = response.prompt_feedback.block_reason if response.prompt_feedback else \"Unknown\"; logging.warning(f\"Gemini Text response blocked/empty. Reason: {block_reason}\"); return None\n",
    "                return response.text.strip()\n",
    "            except google_exceptions.GoogleAPIError as e:\n",
    "                 wait_time = LLM_RETRY_DELAY * (2 ** attempt); logging.warning(f\"Gemini Text API error (Attempt {attempt + 1}/{LLM_RETRY_ATTEMPTS}): {e}. Waiting {wait_time:.2f}s...\")\n",
    "                 if attempt == LLM_RETRY_ATTEMPTS - 1: logging.error(f\"Gemini Text call failed after retries (final error: {e}).\"); return None\n",
    "                 time.sleep(wait_time)\n",
    "            except (google_exceptions.InvalidArgument, ValueError) as e: logging.error(f\"Non-retryable Gemini Text error (InvalidArgument/ValueError): {e}\"); return None\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Unexpected error during Gemini Text call (Attempt {attempt + 1}): {e}\")\n",
    "                if attempt < LLM_RETRY_ATTEMPTS - 1: wait_time = LLM_RETRY_DELAY * (2 ** attempt); logging.warning(f\"Retrying unexpected error in {wait_time:.2f}s...\"); time.sleep(wait_time)\n",
    "                else: return None\n",
    "        return None\n",
    "    except Exception as model_init_error: logging.error(f\"Failed to initialize Gemini model '{LLM_TEXT_MODEL}': {model_init_error}\"); return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4 Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if not text: return \"\"\n",
    "    text = text.lower(); text = re.sub(r'\\s+', ' ', text).strip(); return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5 Section Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_sections_llm(pages_text):\n",
    "    \"\"\"Uses Gemini Text LLM to identify page numbers ONLY for TARGET_SECTIONS.\"\"\" \n",
    "    logging.info(f\"Starting section identification focused on: {', '.join(TARGET_SECTIONS)}\") \n",
    "    if not pages_text or not any(pages_text): logging.warning(\"No text provided for section identification.\"); return {}\n",
    "\n",
    "    formatted_pages = []\n",
    "    for i, text in enumerate(pages_text):\n",
    "        max_len_per_page = 400\n",
    "        processed_text_for_prompt = text\n",
    "        if len(processed_text_for_prompt) > max_len_per_page: processed_text_for_prompt = processed_text_for_prompt[:max_len_per_page//2] + \"...\" + processed_text_for_prompt[-max_len_per_page//2:]\n",
    "        formatted_pages.append(f\"--- Page {i + 1} ---\\n{processed_text_for_prompt}\\n\")\n",
    "    full_text_for_prompt = \"\\n\".join(formatted_pages)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    **Your Role:** Expert pitch deck analyst. Map content to specific key sections.\n",
    "    **Instructions:** Analyze the pitch deck text. Identify page number(s) for ONLY these key sections: {', '.join(TARGET_SECTIONS)}. Output ONLY a valid JSON mapping exact section names (from the list) to lists of page numbers (e.g., [1, 2]). Omit sections not found or not in the target list. No explanations or markdown.\n",
    "    **Example Output:** {{\"Problem\": [2, 3], \"Solution\": [4], \"Team\": [10]}}\n",
    "    ---\n",
    "    **Pitch Deck Text:**\n",
    "    {full_text_for_prompt}\n",
    "    ---\n",
    "    **JSON Output:**\n",
    "    \"\"\"\n",
    "    response = call_gemini_text(prompt, max_output_tokens=LLM_MAX_TOKENS_SECTION_ID)\n",
    "    if not response: logging.error(\"Section ID LLM call failed.\"); return {}\n",
    "\n",
    "    try: \n",
    "        json_match = re.search(r'```json\\s*(\\{.*?\\})\\s*```', response, re.DOTALL | re.IGNORECASE)\n",
    "        if not json_match: json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "        if not json_match: raise json.JSONDecodeError(\"No JSON object found\", response, 0)\n",
    "        json_string = json_match.group(1) if len(json_match.groups()) == 1 else json_match.group(0)\n",
    "        identified_sections_pages = json.loads(json_string)\n",
    "        logging.info(f\"LLM raw identification: {identified_sections_pages}\")\n",
    "\n",
    "        \n",
    "        validated_sections = {}\n",
    "        for section, pages in identified_sections_pages.items():\n",
    "            if section in TARGET_SECTIONS and isinstance(pages, list):\n",
    "                valid_pages = [p - 1 for p in pages if isinstance(p, int) and 0 < p <= len(pages_text)]\n",
    "                if valid_pages: validated_sections[section] = sorted(list(set(valid_pages)))\n",
    "            else:\n",
    "                logging.warning(f\"LLM identified section '{section}' which is not in TARGET_SECTIONS or has invalid format. Skipping.\")\n",
    "        logging.info(f\"Validated section page indices (0-based): {validated_sections}\")\n",
    "        return validated_sections\n",
    "    except json.JSONDecodeError as e: logging.error(f\"Failed to parse JSON for section ID: {e}. Response: {response}\"); return {}\n",
    "    except Exception as e: logging.error(f\"Error processing section ID response: {e}\"); return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.6 Aggregate Section Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_section_content(pages_text, section_pages_map):\n",
    "    section_content = {};\n",
    "    if not section_pages_map: return {}\n",
    "    logging.info(\"Aggregating text content for identified sections...\")\n",
    "    for section, page_indices in section_pages_map.items():\n",
    "        content = \"\".join(pages_text[idx] + \"\\n\\n\" for idx in page_indices if 0 <= idx < len(pages_text))\n",
    "        section_content[section] = preprocess_text(content) # Preprocess before scoring\n",
    "    logging.info(f\"Aggregated content for sections: {list(section_content.keys())}\")\n",
    "    return section_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.7 Score Sections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sections_llm(section_content):\n",
    "    logging.info(\"Starting section scoring...\")\n",
    "    section_scores = {};\n",
    "    if not section_content: return {}\n",
    "    scoring_instructions = \"\"\"**Your Role:** Expert pitch deck analyst. Evaluate section text based *only* on given criteria. **Instructions:** Provide numerical score (0-100) and brief justification (2-3 sentences). Format output ONLY as valid JSON: {\"score\": integer, \"justification\": string}. No explanations/markdown.\"\"\"\n",
    "    for section, text in section_content.items():\n",
    "        if section in SCORING_CRITERIA: # Only score sections with defined criteria\n",
    "            logging.info(f\"Scoring section: {section}\")\n",
    "            if not text or len(text.strip()) < 20:\n",
    "                logging.warning(f\"Skipping scoring for '{section}' (insufficient content).\"); section_scores[section] = {\"score\": 0, \"justification\": \"Content missing or too brief.\"}; continue\n",
    "            prompt = f\"\"\"{scoring_instructions}\\n---\\n**Section:** {section}\\n**Criteria:**\\n{SCORING_CRITERIA[section]}\\n---\\n**Text:**\\n{text[:4000]}\\n---\\n**JSON Output:**\"\"\"\n",
    "            response = call_gemini_text(prompt, max_output_tokens=LLM_MAX_TOKENS_SCORING)\n",
    "            if response:\n",
    "                 try:\n",
    "                    json_match = re.search(r'```json\\s*(\\{.*?\\})\\s*```', response, re.DOTALL | re.IGNORECASE);\n",
    "                    if not json_match: json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "                    if not json_match: raise json.JSONDecodeError(\"No JSON object found\", response, 0)\n",
    "                    json_string = json_match.group(1) if len(json_match.groups()) == 1 else json_match.group(0)\n",
    "                    score_data = json.loads(json_string)\n",
    "                    if isinstance(score_data.get('score'), int) and 0 <= score_data['score'] <= 100 and isinstance(score_data.get('justification'), str):\n",
    "                        section_scores[section] = score_data; logging.info(f\"Scored '{section}': {score_data['score']}/100\")\n",
    "                    else: logging.warning(f\"LLM scoring response invalid format/values for '{section}'. Parsed: {score_data}. Resp: {response}\"); section_scores[section] = {\"score\": 0, \"justification\": \"Failed parsing score (invalid format/values).\"}\n",
    "                 except json.JSONDecodeError as e: logging.warning(f\"Failed JSON decode for '{section}' scoring: {e}. Resp: {response}\"); section_scores[section] = {\"score\": 0, \"justification\": \"Failed parsing score (JSON decode error).\"}\n",
    "                 except Exception as e: logging.error(f\"Unexpected error processing score for '{section}': {e}\"); section_scores[section] = {\"score\": 0, \"justification\": f\"Internal error scoring.\"}\n",
    "            else: logging.error(f\"LLM call failed for scoring '{section}'.\"); section_scores[section] = {\"score\": 0, \"justification\": \"LLM call failed during scoring.\"}\n",
    "    logging.info(\"Finished section scoring.\")\n",
    "    return section_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.8 Calculate Overall Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overall_score(section_scores, weights):\n",
    "    total_score_points = 0; total_weight = 0;\n",
    "    if not section_scores or not weights: return 0\n",
    "    logging.info(\"Calculating overall weighted score based on target sections...\")\n",
    "    for section, score_data in section_scores.items():\n",
    "        if section in weights: # Only consider sections we defined weights for\n",
    "             weight = weights[section]\n",
    "             total_weight += weight # Increment total weight considered\n",
    "             if isinstance(score_data.get('score'), int) and score_data['score'] >= 0: # Include 0 scores in normalization\n",
    "                  score = score_data['score']\n",
    "                  total_score_points += score * weight # Weighted score contribution\n",
    "             else:\n",
    "                  logging.warning(f\"Section '{section}' has weight but invalid score ({score_data.get('score')}). Excluding score contribution.\")\n",
    "\n",
    "    if total_weight > 0:\n",
    "        normalized_score = (total_score_points / total_weight) if total_weight > 0 else 0\n",
    "        logging.info(f\"Raw weighted score sum: {total_score_points:.0f}, Total weight considered: {total_weight}\")\n",
    "        logging.info(f\"Normalized overall score: {normalized_score:.0f}\")\n",
    "        return round(normalized_score)\n",
    "    else:\n",
    "        logging.warning(\"No target sections with defined weights were found or scored. Overall score is 0.\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.9 Generate Qualitative Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feedback_llm(section_scores):\n",
    "    \"\"\"Uses Gemini Text LLM to generate feedback focused on TARGET_SECTIONS.\"\"\"\n",
    "    logging.info(\"Generating qualitative feedback focused on target sections...\")\n",
    "    if not section_scores: return {\"strengths\": [], \"weaknesses\": []}\n",
    "\n",
    "    analysis_summary = \"\"\n",
    "    valid_scores_found = False\n",
    "    target_sections_found_count = 0\n",
    "    for section in TARGET_SECTIONS: \n",
    "        if section in section_scores:\n",
    "            target_sections_found_count += 1\n",
    "            data = section_scores[section]\n",
    "            score = data.get('score')\n",
    "            justification = data.get('justification', 'N/A')\n",
    "            if isinstance(score, int):\n",
    "                 analysis_summary += f\"Section: {section}\\nScore: {score}/100\\nJustification: {justification}\\n---\\n\"\n",
    "                 if score > 0 or \"missing\" not in justification.lower(): valid_scores_found = True\n",
    "        else:\n",
    "             analysis_summary += f\"Section: {section}\\nScore: Not Found/Identified\\n---\\n\"\n",
    "\n",
    "\n",
    "    if target_sections_found_count == 0:\n",
    "         logging.warning(\"None of the target sections were found in the analysis.\")\n",
    "         return {\"strengths\": [\"Analysis failed: None of the core sections (Problem, Solution, Market, etc.) were identified.\"], \"weaknesses\": []}\n",
    "    if not valid_scores_found:\n",
    "         logging.warning(\"No scorable target sections found to generate meaningful feedback.\")\n",
    "         return {\"strengths\": [\"Analysis incomplete: Core sections found but could not be scored.\"], \"weaknesses\": []}\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    **Your Role:** Constructive startup pitch analyst. Synthesize analysis of core sections into actionable feedback.\n",
    "    **Analysis Summary (Focusing on Problem, Solution, Market, Business Model, Financials, Team):**\n",
    "    ---\n",
    "    {analysis_summary}\n",
    "    ---\n",
    "    **Instructions:**\n",
    "    1. Identify 2-3 key STRENGTHS based *primarily* on the analysis of the core sections listed above.\n",
    "    2. Identify 2-3 critical WEAKNESSES based *primarily* on the analysis of these core sections.\n",
    "    3. For each weakness, provide a specific, actionable suggestion for improvement.\n",
    "    4. Format output ONLY as a valid JSON object: {{\"strengths\": [list of strings], \"weaknesses\": [list of strings]}}. No explanations or markdown.\n",
    "    **Example Output:** {{\"strengths\": [\"Strong problem validation (Problem: 90).\", \"Clear value prop (Solution: 80).\"], \"weaknesses\": [\"Market Size lacks sources (Market: 40). Suggestion: Cite specific market reports.\", \"Financial assumptions unclear (Financials: 55). Suggestion: Detail key drivers.\"]}}\n",
    "    ---\n",
    "    **JSON Output:**\n",
    "    \"\"\"\n",
    "    response = call_gemini_text(prompt, max_output_tokens=LLM_MAX_TOKENS_FEEDBACK)\n",
    "    default_error = {\"strengths\": [\"Feedback generation failed.\"], \"weaknesses\": []}\n",
    "    if not response: logging.error(\"Feedback LLM call failed.\"); return default_error\n",
    "\n",
    "    try: \n",
    "        json_match = re.search(r'```json\\s*(\\{.*?\\})\\s*```', response, re.DOTALL | re.IGNORECASE)\n",
    "        if not json_match: json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "        if not json_match: raise json.JSONDecodeError(\"No JSON object found\", response, 0)\n",
    "        json_string = json_match.group(1) if len(json_match.groups()) == 1 else json_match.group(0)\n",
    "        feedback_data = json.loads(json_string)\n",
    "        if isinstance(feedback_data.get('strengths'), list) and isinstance(feedback_data.get('weaknesses'), list):\n",
    "            logging.info(\"Successfully generated qualitative feedback.\")\n",
    "            return feedback_data\n",
    "        else: logging.error(f\"Feedback LLM response invalid format. Parsed: {feedback_data}. Resp: {response}\"); return {\"strengths\": [\"Failed parsing feedback (invalid format).\"],\"weaknesses\": []}\n",
    "    except json.JSONDecodeError as e: logging.error(f\"Failed JSON decode for feedback: {e}. Resp: {response}\"); return {\"strengths\": [\"Failed parsing feedback (JSON decode error).\"],\"weaknesses\": []}\n",
    "    except Exception as e: logging.error(f\"Unexpected error processing feedback response: {e}\"); return default_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Main Workflow Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_pitch_deck(pdf_path):\n",
    "    \"\"\"Orchestrates the focused pitch deck analysis process.\"\"\"\n",
    "    results = {\n",
    "        'overall_score': 0, 'section_scores': {}, 'feedback': {'strengths': [], 'weaknesses': []},\n",
    "        'raw_text_pages': [], 'section_mapping': {}, 'error': None\n",
    "    }\n",
    "    start_time_analysis = time.time()\n",
    "    logging.info(f\"--- Starting Focused Pitch Deck Analysis (6 Sections) for: {pdf_path} ---\")\n",
    "\n",
    "    # Text Extraction\n",
    "    logging.info(\"Step 1: Extracting Text using Gemini OCR...\")\n",
    "    raw_pages_text = extract_text_with_ocr_and_gemini(pdf_path)\n",
    "    if raw_pages_text is None: results['error'] = \"Critical error during PDF/OCR.\"; logging.error(results['error']); return results\n",
    "    if not any(page_text.strip() for page_text in raw_pages_text): results['error'] = \"Failed to extract any text content.\"; logging.error(results['error']); return results\n",
    "    successful_extractions = sum(1 for t in raw_pages_text if t.strip()); logging.info(f\"Extracted text from {successful_extractions}/{len(raw_pages_text)} pages via OCR.\")\n",
    "    results['raw_text_pages'] = raw_pages_text\n",
    "\n",
    "    # Section Identification \n",
    "    logging.info(\"Step 2: Identifying Target Sections...\")\n",
    "    section_pages_map = identify_sections_llm(raw_pages_text) # Function now uses TARGET_SECTIONS in prompt\n",
    "    if not section_pages_map: logging.warning(\"Could not identify any of the target sections. Analysis will be limited.\"); # Continue? Or stop? Let's continue but score/feedback will be minimal.\n",
    "    results['section_mapping'] = section_pages_map\n",
    "\n",
    "    # Aggregate & Preprocess Section Content\n",
    "    logging.info(\"Step 3: Aggregating and Preprocessing Section Content...\")\n",
    "    section_content = aggregate_section_content(raw_pages_text, section_pages_map)\n",
    "    if not section_content and section_pages_map: logging.warning(\"Section mapping found, but failed to aggregate content.\")\n",
    "\n",
    "    # Scoring Sections \n",
    "    logging.info(\"Step 4: Scoring Sections...\")\n",
    "    section_scores = score_sections_llm(section_content)\n",
    "    results['section_scores'] = section_scores\n",
    "    if not section_scores: logging.warning(\"Section scoring produced no results.\")\n",
    "\n",
    "    # Calculate Overall Score \n",
    "    logging.info(\"Step 5: Calculating Overall Score...\")\n",
    "    overall_score = calculate_overall_score(section_scores, SECTION_WEIGHTS)\n",
    "    results['overall_score'] = overall_score\n",
    "\n",
    "    # Generate Qualitative Feedback \n",
    "    logging.info(\"Step 6: Generating Feedback...\")\n",
    "    feedback = generate_feedback_llm(section_scores) # Function now focuses prompt\n",
    "    results['feedback'] = feedback\n",
    "\n",
    "    total_time = time.time() - start_time_analysis\n",
    "    logging.info(f\"--- Focused Analysis Completed in {total_time:.2f} seconds ---\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-30 18:07:53,209 - INFO - --- Starting Focused Pitch Deck Analysis (6 Sections) for: ../data/Uber-Pitch-Deck.pdf ---\n",
      "2025-03-30 18:07:53,210 - INFO - Step 1: Extracting Text using Gemini OCR...\n",
      "2025-03-30 18:07:53,210 - INFO - Starting PDF processing with Gemini OCR for: ../data/Uber-Pitch-Deck.pdf\n",
      "2025-03-30 18:07:53,212 - INFO - Converting PDF to images...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      " AI Pitch Deck Analyzer (Focused on 6 Key Sections)\n",
      "============================================================\n",
      "Target Sections: Problem, Solution, Market Size, Business Model, Financial Projections, Team\n",
      "Processing file: ../data/Uber-Pitch-Deck.pdf\n",
      "Text Model: gemini-1.5-flash-latest, Vision Model: gemini-1.5-pro-latest\n",
      "Inter-Page OCR Delay: 10s\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-30 18:08:02,321 - INFO - Converted 24 pages in 9.11s.\n",
      "2025-03-30 18:08:02,323 - INFO - Processing page 1/24 with Gemini Vision OCR...\n",
      "2025-03-30 18:08:05,442 - INFO - Page 1 OCR took 3.12s.\n",
      "2025-03-30 18:08:05,442 - INFO - Waiting 10s before next page...\n",
      "2025-03-30 18:08:15,456 - INFO - Processing page 2/24 with Gemini Vision OCR...\n",
      "2025-03-30 18:08:19,326 - INFO - Page 2 OCR took 3.87s.\n",
      "2025-03-30 18:08:19,326 - INFO - Waiting 10s before next page...\n",
      "2025-03-30 18:08:29,340 - INFO - Processing page 3/24 with Gemini Vision OCR...\n",
      "2025-03-30 18:08:33,289 - INFO - Page 3 OCR took 3.95s.\n",
      "2025-03-30 18:08:33,289 - INFO - Waiting 10s before next page...\n",
      "2025-03-30 18:08:43,290 - INFO - Processing page 4/24 with Gemini Vision OCR...\n",
      "2025-03-30 18:08:47,368 - INFO - Page 4 OCR took 4.08s.\n",
      "2025-03-30 18:08:47,370 - INFO - Waiting 10s before next page...\n",
      "2025-03-30 18:08:57,371 - INFO - Processing page 5/24 with Gemini Vision OCR...\n",
      "2025-03-30 18:08:58,851 - WARNING - Gemini API rate limit/quota error (Attempt 1/3). Waiting 15.00s...\n",
      "2025-03-30 18:09:17,206 - INFO - Page 5 OCR took 19.83s.\n",
      "2025-03-30 18:09:17,206 - INFO - Waiting 10s before next page...\n",
      "2025-03-30 18:09:27,216 - INFO - Processing page 6/24 with Gemini Vision OCR...\n",
      "2025-03-30 18:09:31,839 - INFO - Page 6 OCR took 4.62s.\n",
      "2025-03-30 18:09:31,839 - INFO - Waiting 10s before next page...\n",
      "2025-03-30 18:09:41,853 - INFO - Processing page 7/24 with Gemini Vision OCR...\n",
      "2025-03-30 18:09:45,919 - INFO - Page 7 OCR took 4.07s.\n",
      "2025-03-30 18:09:45,919 - INFO - Waiting 10s before next page...\n",
      "2025-03-30 18:09:55,922 - INFO - Processing page 8/24 with Gemini Vision OCR...\n",
      "2025-03-30 18:09:57,086 - WARNING - Gemini API rate limit/quota error (Attempt 1/3). Waiting 15.00s...\n",
      "2025-03-30 18:10:15,739 - INFO - Page 8 OCR took 19.82s.\n",
      "2025-03-30 18:10:15,739 - INFO - Waiting 10s before next page...\n",
      "2025-03-30 18:10:25,753 - INFO - Processing page 9/24 with Gemini Vision OCR...\n",
      "2025-03-30 18:10:31,270 - INFO - Page 9 OCR took 5.52s.\n",
      "2025-03-30 18:10:31,270 - INFO - Waiting 10s before next page...\n",
      "2025-03-30 18:10:41,270 - INFO - Processing page 10/24 with Gemini Vision OCR...\n",
      "2025-03-30 18:10:44,256 - INFO - Page 10 OCR took 2.99s.\n",
      "2025-03-30 18:10:44,256 - INFO - Waiting 10s before next page...\n",
      "2025-03-30 18:10:54,269 - INFO - Processing page 11/24 with Gemini Vision OCR...\n",
      "2025-03-30 18:10:57,486 - INFO - Page 11 OCR took 3.22s.\n",
      "2025-03-30 18:10:57,486 - INFO - Waiting 10s before next page...\n",
      "2025-03-30 18:11:07,486 - INFO - Processing page 12/24 with Gemini Vision OCR...\n",
      "2025-03-30 18:11:12,635 - INFO - Page 12 OCR took 5.15s.\n",
      "2025-03-30 18:11:12,635 - INFO - Waiting 10s before next page...\n",
      "2025-03-30 18:11:22,636 - INFO - Processing page 13/24 with Gemini Vision OCR...\n",
      "2025-03-30 18:11:25,452 - INFO - Page 13 OCR took 2.82s.\n",
      "2025-03-30 18:11:25,452 - INFO - Waiting 10s before next page...\n",
      "2025-03-30 18:11:35,466 - INFO - Processing page 14/24 with Gemini Vision OCR...\n",
      "2025-03-30 18:11:38,270 - INFO - Page 14 OCR took 2.80s.\n",
      "2025-03-30 18:11:38,270 - INFO - Waiting 10s before next page...\n",
      "2025-03-30 18:11:48,284 - INFO - Processing page 15/24 with Gemini Vision OCR...\n",
      "2025-03-30 18:11:49,020 - WARNING - Gemini API rate limit/quota error (Attempt 1/3). Waiting 15.00s...\n",
      "2025-03-30 18:12:06,617 - INFO - Page 15 OCR took 18.33s.\n",
      "2025-03-30 18:12:06,617 - INFO - Waiting 10s before next page...\n",
      "2025-03-30 18:12:16,632 - INFO - Processing page 16/24 with Gemini Vision OCR...\n",
      "2025-03-30 18:12:19,517 - INFO - Page 16 OCR took 2.88s.\n",
      "2025-03-30 18:12:19,517 - INFO - Waiting 10s before next page...\n",
      "2025-03-30 18:12:29,533 - INFO - Processing page 17/24 with Gemini Vision OCR...\n",
      "2025-03-30 18:12:32,317 - INFO - Page 17 OCR took 2.78s.\n",
      "2025-03-30 18:12:32,317 - INFO - Waiting 10s before next page...\n",
      "2025-03-30 18:12:42,317 - INFO - Processing page 18/24 with Gemini Vision OCR...\n",
      "2025-03-30 18:12:43,228 - WARNING - Gemini API rate limit/quota error (Attempt 1/3). Waiting 15.00s...\n",
      "2025-03-30 18:12:59,102 - WARNING - Gemini API rate limit/quota error (Attempt 2/3). Waiting 30.00s...\n",
      "2025-03-30 18:13:32,565 - INFO - Page 18 OCR took 50.25s.\n",
      "2025-03-30 18:13:32,565 - INFO - Waiting 10s before next page...\n",
      "2025-03-30 18:13:42,581 - INFO - Processing page 19/24 with Gemini Vision OCR...\n",
      "2025-03-30 18:13:45,432 - INFO - Page 19 OCR took 2.85s.\n",
      "2025-03-30 18:13:45,448 - INFO - Waiting 10s before next page...\n",
      "2025-03-30 18:13:55,465 - INFO - Processing page 20/24 with Gemini Vision OCR...\n",
      "2025-03-30 18:13:59,549 - INFO - Page 20 OCR took 4.08s.\n",
      "2025-03-30 18:13:59,549 - INFO - Waiting 10s before next page...\n",
      "2025-03-30 18:14:09,580 - INFO - Processing page 21/24 with Gemini Vision OCR...\n",
      "2025-03-30 18:14:12,637 - INFO - Page 21 OCR took 3.06s.\n",
      "2025-03-30 18:14:12,637 - INFO - Waiting 10s before next page...\n",
      "2025-03-30 18:14:22,647 - INFO - Processing page 22/24 with Gemini Vision OCR...\n",
      "2025-03-30 18:14:25,666 - INFO - Page 22 OCR took 3.02s.\n",
      "2025-03-30 18:14:25,680 - INFO - Waiting 10s before next page...\n",
      "2025-03-30 18:14:35,682 - INFO - Processing page 23/24 with Gemini Vision OCR...\n",
      "2025-03-30 18:14:39,148 - INFO - Page 23 OCR took 3.47s.\n",
      "2025-03-30 18:14:39,148 - INFO - Waiting 10s before next page...\n",
      "2025-03-30 18:14:49,153 - INFO - Processing page 24/24 with Gemini Vision OCR...\n",
      "2025-03-30 18:14:49,948 - WARNING - Gemini API rate limit/quota error (Attempt 1/3). Waiting 15.00s...\n",
      "2025-03-30 18:15:08,485 - INFO - Page 24 OCR took 19.33s.\n",
      "2025-03-30 18:15:08,487 - INFO - Completed Gemini OCR processing.\n",
      "2025-03-30 18:15:08,488 - INFO - Extracted text from 24/24 pages via OCR.\n",
      "2025-03-30 18:15:08,488 - INFO - Step 2: Identifying Target Sections...\n",
      "2025-03-30 18:15:08,488 - INFO - Starting section identification focused on: Problem, Solution, Market Size, Business Model, Financial Projections, Team\n",
      "2025-03-30 18:15:09,465 - INFO - LLM raw identification: {'Problem': [2, 3], 'Solution': [4, 5, 6, 7], 'Market Size': [15, 16, 17, 19], 'Business Model': [7], 'Financial Projections': [18], 'Team': [24]}\n",
      "2025-03-30 18:15:09,465 - INFO - Validated section page indices (0-based): {'Problem': [1, 2], 'Solution': [3, 4, 5, 6], 'Market Size': [14, 15, 16, 18], 'Business Model': [6], 'Financial Projections': [17], 'Team': [23]}\n",
      "2025-03-30 18:15:09,465 - INFO - Step 3: Aggregating and Preprocessing Section Content...\n",
      "2025-03-30 18:15:09,465 - INFO - Aggregating text content for identified sections...\n",
      "2025-03-30 18:15:09,465 - INFO - Aggregated content for sections: ['Problem', 'Solution', 'Market Size', 'Business Model', 'Financial Projections', 'Team']\n",
      "2025-03-30 18:15:09,465 - INFO - Step 4: Scoring Sections...\n",
      "2025-03-30 18:15:09,465 - INFO - Starting section scoring...\n",
      "2025-03-30 18:15:09,465 - INFO - Scoring section: Problem\n",
      "2025-03-30 18:15:10,451 - INFO - Scored 'Problem': 70/100\n",
      "2025-03-30 18:15:10,451 - INFO - Scoring section: Solution\n",
      "2025-03-30 18:15:11,481 - INFO - Scored 'Solution': 70/100\n",
      "2025-03-30 18:15:11,481 - INFO - Scoring section: Market Size\n",
      "2025-03-30 18:15:12,482 - INFO - Scored 'Market Size': 45/100\n",
      "2025-03-30 18:15:12,482 - INFO - Scoring section: Business Model\n",
      "2025-03-30 18:15:13,382 - INFO - Scored 'Business Model': 45/100\n",
      "2025-03-30 18:15:13,382 - INFO - Scoring section: Financial Projections\n",
      "2025-03-30 18:15:14,398 - INFO - Scored 'Financial Projections': 30/100\n",
      "2025-03-30 18:15:14,398 - INFO - Scoring section: Team\n",
      "2025-03-30 18:15:15,348 - INFO - Scored 'Team': 45/100\n",
      "2025-03-30 18:15:15,364 - INFO - Finished section scoring.\n",
      "2025-03-30 18:15:15,365 - INFO - Step 5: Calculating Overall Score...\n",
      "2025-03-30 18:15:15,366 - INFO - Calculating overall weighted score based on target sections...\n",
      "2025-03-30 18:15:15,366 - INFO - Raw weighted score sum: 5290, Total weight considered: 100\n",
      "2025-03-30 18:15:15,366 - INFO - Normalized overall score: 53\n",
      "2025-03-30 18:15:15,366 - INFO - Step 6: Generating Feedback...\n",
      "2025-03-30 18:15:15,366 - INFO - Generating qualitative feedback focused on target sections...\n",
      "2025-03-30 18:15:16,865 - INFO - Successfully generated qualitative feedback.\n",
      "2025-03-30 18:15:16,865 - INFO - --- Focused Analysis Completed in 443.66 seconds ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================== ANALYSIS RESULTS ==============================\n",
      "\n",
      "Overall Pitch Score (Based on Weighted Sections): 53/100\n",
      "\n",
      "----------------------------------------\n",
      "Section Scores & Justifications:\n",
      "----------------------------------------\n",
      "  - Problem (Weight: 20): 70/100\n",
      "    Justification: The problem of inefficient taxi technology in 2008 is clearly stated and its magnitude is supported by data points like fuel consumption and medallion costs.  However, the urgency aspect is less emphasized, and the target audience (passengers and drivers) could be more precisely defined.\n",
      "  - Solution (Weight: 20): 70/100\n",
      "    Justification: The solution is clearly explained, directly addresses the problem of inefficient taxi services, and offers a compelling value proposition of convenience and luxury.  However, feasibility and scalability aspects lack detail, and the text is somewhat disorganized.\n",
      "  - Market Size (Weight: 13): 45/100\n",
      "    Justification: The text partially defines the market (TAM is mentioned), but lacks clear definitions of SAM and SOM.  Data is presented without sources, and the growth projection lacks specifics and credible support.  The focus on a limited geographic area initially is realistic, but the overall market size estimation needs significant improvement.\n",
      "  - Business Model (Weight: 13): 45/100\n",
      "    Justification: The text mentions a pre-paid, cashless billing system (revenue stream), but lacks detail on pricing strategy and profitability.  Scalability is hinted at with fleet size, but needs more concrete explanation.  Unit economics are entirely absent.\n",
      "  - Financial Projections (Weight: 14): 30/100\n",
      "    Justification: The projections lack clarity and key metrics such as costs and burn rate.  Assumptions are vaguely stated and realism is questionable due to the absence of market analysis and justification for revenue projections.  The link to funding ask is missing entirely.\n",
      "  - Team (Weight: 20): 45/100\n",
      "    Justification: The team section lacks detail on team members' experience and roles.  While advisors and clients are mentioned, their relevance isn't elaborated.  The focus is on early traction, not team capabilities.\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "Feedback (Focused on Core Sections):\n",
      "----------------------------------------\n",
      "Strengths:\n",
      "  + Clearly defined problem in the context of inefficient 2008 taxi technology.\n",
      "  + Compelling value proposition of convenience and luxury.\n",
      "\n",
      "Weaknesses & Suggestions:\n",
      "  - Market analysis lacks depth and credible data. Suggestion: Define SAM and SOM, cite data sources for TAM, provide realistic growth projections with supporting rationale.\n",
      "  - Financial projections are insufficient and lack key metrics. Suggestion: Develop detailed financial projections including cost structure, burn rate, unit economics, and clearly link to funding ask.\n",
      "  - Team section lacks detail on member expertise and roles. Suggestion: Provide detailed biographies of key team members, highlighting relevant experience and clearly defining roles and responsibilities.\n",
      "\n",
      "==============================================================================\n",
      "\n",
      "Analysis complete.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    pdf_file_path = '../data/Uber-Pitch-Deck.pdf' # Example\n",
    "\n",
    "    print(\"=\"*60); print(\" AI Pitch Deck Analyzer (Focused on 6 Key Sections)\"); print(\"=\"*60)\n",
    "    print(f\"Target Sections: {', '.join(TARGET_SECTIONS)}\")\n",
    "    print(f\"Processing file: {pdf_file_path}\")\n",
    "    print(f\"Text Model: {LLM_TEXT_MODEL}, Vision Model: {LLM_VISION_MODEL}\")\n",
    "    print(f\"Inter-Page OCR Delay: {INTER_PAGE_DELAY}s\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    if not os.path.exists(pdf_file_path): print(f\"\\nERROR: PDF file not found: '{pdf_file_path}'.\")\n",
    "    elif not genai_configured: print(\"\\nERROR: Google Generative AI client not configured.\")\n",
    "    else:\n",
    "        analysis_results = analyze_pitch_deck(pdf_file_path)\n",
    "\n",
    "        # --- Display Results ---\n",
    "        print(\"\\n\" + \"=\"*30 + \" ANALYSIS RESULTS \" + \"=\"*30)\n",
    "\n",
    "        if analysis_results.get('error'): print(f\"\\nAnalysis Failed: {analysis_results['error']}\\n\")\n",
    "        else:\n",
    "            \n",
    "            print(f\"\\nOverall Pitch Score (Based on Weighted Sections): {analysis_results['overall_score']}/100\\n\")\n",
    "\n",
    "            print(\"-\" * 40); print(\"Section Scores & Justifications:\"); print(\"-\" * 40)\n",
    "            if analysis_results['section_scores']:\n",
    "                 \n",
    "                 target_sections_found_in_scores = False\n",
    "                 for section in TARGET_SECTIONS:\n",
    "                     if section in analysis_results['section_scores']:\n",
    "                         target_sections_found_in_scores = True\n",
    "                         data = analysis_results['section_scores'][section]\n",
    "                         score = data.get('score', 'N/A')\n",
    "                         justification = data.get('justification', 'N/A')\n",
    "                         weight_info = f\"(Weight: {SECTION_WEIGHTS.get(section, 0)})\" # Get weight safely\n",
    "                         print(f\"  - {section} {weight_info}: {score}/100\")\n",
    "                         print(f\"    Justification: {justification}\")\n",
    "                     else:\n",
    "                         print(f\"  - {section} (Weight: {SECTION_WEIGHTS.get(section, 0)}): Not Found/Scored\")\n",
    "                 if not target_sections_found_in_scores:\n",
    "                      print(\"  None of the target sections were identified or scored.\")\n",
    "            else: print(\"  No sections were scored.\")\n",
    "            print(\"\\n\")\n",
    "\n",
    "            print(\"-\" * 40); print(\"Feedback (Focused on Core Sections):\"); print(\"-\" * 40)\n",
    "            feedback = analysis_results.get('feedback', {}); strengths = feedback.get('strengths'); weaknesses = feedback.get('weaknesses')\n",
    "            print(\"Strengths:\")\n",
    "            if strengths: [print(f\"  + {s}\") for s in strengths]\n",
    "            else: print(\"  No specific strengths identified.\")\n",
    "            print(\"\\nWeaknesses & Suggestions:\")\n",
    "            if weaknesses: [print(f\"  - {w}\") for w in weaknesses]\n",
    "            else: print(\"  No specific weaknesses identified.\")\n",
    "            print(\"\\n\" + \"=\"*78 + \"\\n\")\n",
    "\n",
    "    print(\"Analysis complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
